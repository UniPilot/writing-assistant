import streamlit as st
import subprocess
from pypinyin import pinyin, Style
import spacy
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel
from pymongo import MongoClient
from pymongo.server_api import ServerApi
import certifi
import random

# ====== åŠ è½½ä¸­æ–‡æ¨¡å‹ ======
nlp = spacy.load("zh_core_web_sm")
ENABLE_SELF_REFLECTION = True

# ====== æœ¬åœ° Qwen è°ƒç”¨å‡½æ•° ======
def call_local_qwen(prompt: str) -> str:
    try:
        process = subprocess.Popen(
            ['ollama', 'run', 'qwen2.5:14b'],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding='utf-8'
        )
        output, error = process.communicate(prompt)
        if process.returncode != 0:
            return f"[æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥] {error.strip()}"
        return output.strip()
    except Exception as e:
        return f"[æœ¬åœ°æ¨¡å‹è°ƒç”¨å‡ºé”™] {str(e)}"

# ====== MongoDB è¿æ¥å‡½æ•° ======
def connect_mongodb():
    uri = "mongodb+srv://2068432802:lzq520796@cluster0.tmy62.mongodb.net/?retryWrites=true&w=majority"
    client = MongoClient(uri,
                         tlsCAFile=certifi.where(),
                         server_api=ServerApi('1'))
    try:
        client.admin.command('ping')
        print("æˆåŠŸè¿æ¥åˆ° MongoDB!")
        return client
    except Exception as e:
        print("è¿æ¥å¤±è´¥:", e)
        return None

# ====== BERT åˆå§‹åŒ– ======
def initialize_bert():
    try:
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        model = BertModel.from_pretrained('bert-base-uncased')
        return tokenizer, model
    except Exception as e:
        print(f"BERT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

# ====== è®¡ç®—æ–‡æœ¬åµŒå…¥ ======
def get_embedding(text, tokenizer, model):
    tokens = tokenizer(text,
                       return_tensors='pt',
                       truncation=True,
                       padding=True,
                       max_length=512)
    with torch.no_grad():
        outputs = model(**tokens)
    return outputs.last_hidden_state[:, 0, :]

# ====== ç›¸ä¼¼åº¦æœç´¢ ======
def find_most_similar(input_text, collection, tokenizer, model):
    input_embedding = get_embedding(input_text, tokenizer, model)

    max_similarity = -1
    most_similar_article = None

    for doc in collection.find():
        if 'content' not in doc:
            continue

        db_embedding = get_embedding(doc['content'], tokenizer, model)
        similarity = F.cosine_similarity(input_embedding, db_embedding).item()

        if similarity > max_similarity:
            max_similarity = similarity
            most_similar_article = doc

    return most_similar_article, max_similarity

# ====== éšæœºé€‰æ–‡ ======
def find_random_article(collection):
    docs = list(collection.find({"content": {"$exists": True}}))
    return random.choice(docs) if docs else None

# ====== é£æ ¼è¿ç§»ï¼ˆè°ƒç”¨æœ¬åœ°æ¨¡å‹ï¼‰ ======
def adjust_writing_style_local(input_text, reference_text):
    prompt = (
        "ä½ æ˜¯ä¸€åå­¦æœ¯å†™ä½œä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®å‚è€ƒè®ºæ–‡è°ƒæ•´æ–‡æœ¬é£æ ¼ã€‚è¯·å°½æœ€å¤§å¯èƒ½ä¿æŒåŸæ–‡å†…å®¹ï¼Œ"
        "ä¸è¦å¢åŠ åŸæ–‡æ²¡æœ‰çš„å†…å®¹ï¼Œåªä¿®æ”¹å†™ä½œé£æ ¼ï¼Œä½†ä¿®æ”¹ç”¨è¯ã€å¥å¼å’Œç»“æ„ä»¥å°½é‡åŒ¹é…å‚è€ƒè®ºæ–‡çš„å­¦æœ¯é£æ ¼ã€‚\n"
        f"å‚è€ƒè®ºæ–‡ç‰‡æ®µï¼š\n{reference_text[:2000]}...\n\n"
        f"è¯·ä¿®æ”¹ä»¥ä¸‹æ–‡ç« ä½¿å…¶ç¬¦åˆå‚è€ƒè®ºæ–‡çš„å­¦æœ¯é£æ ¼ï¼š\n{input_text}"
    )
    return call_local_qwen(prompt)

# ====== è¯­æ³•åˆ†æ ======
def generate_syntax_analysis(text):
    doc = nlp(text)
    lines = ["[ä¾å­˜å¥æ³•åˆ†æ]"]
    for token in doc:
        lines.append(f"è¯è¯­: {token.text:<5} è¯æ€§: {token.pos_:<5} ä¾å­˜å…³ç³»: {token.dep_:<10} æ”¯é…è¯: {token.head.text}")
    lines.append("\n[å‘½åå®ä½“è¯†åˆ«]")
    if not doc.ents:
        lines.append("æœªè¯†åˆ«åˆ°å‘½åå®ä½“")
    else:
        for ent in doc.ents:
            lines.append(f"å®ä½“: {ent.text:<15} ç±»å‹: {ent.label_:<10} ä½ç½®: ({ent.start_char}-{ent.end_char})")
    return "\n".join(lines)

# ====== è·å–æ‹¼éŸ³ ======
def get_pinyin_with_tone(text):
    pinyin_list = pinyin(text, style=Style.TONE3)
    return " ".join([item[0] for item in pinyin_list])

# ====== Streamlit App ======
def main():
    st.title("ğŸ“ ä¸­æ–‡æ–‡æœ¬åŠ©æ‰‹")

    # è¿æ¥ MongoDB å’Œ BERT æ¨¡å‹ï¼ŒAppå¯åŠ¨æ—¶åªåšä¸€æ¬¡
    if "mongodb_client" not in st.session_state:
        st.session_state.mongodb_client = connect_mongodb()
    if "bert_tokenizer" not in st.session_state or "bert_model" not in st.session_state:
        tokenizer, model = initialize_bert()
        st.session_state.bert_tokenizer = tokenizer
        st.session_state.bert_model = model

    # åŠŸèƒ½é€‰é¡¹å¡
    feature = st.radio("è¯·é€‰æ‹©åŠŸèƒ½", ["é£æ ¼è¿ç§»", "è¯­ä¹‰çº é”™"], horizontal=True)

    # è¾“å…¥æ¡†
    user_input = st.text_area(f"è¯·è¾“å…¥æ–‡æœ¬ï¼ˆå½“å‰åŠŸèƒ½ï¼š{feature}ï¼‰", height=150)

    if st.button("æ‰§è¡Œ"):
        if not user_input.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬ï¼")
            return

        if feature == "é£æ ¼è¿ç§»":
            if not st.session_state.mongodb_client or not st.session_state.bert_tokenizer or not st.session_state.bert_model:
                st.error("æ•°æ®åº“æˆ–æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–")
                return

            db = st.session_state.mongodb_client.get_database("paper")
            collection = db.get_collection("papers")

            # æ‰¾ç›¸ä¼¼å‚è€ƒè®ºæ–‡
            ref_doc, sim_score = find_most_similar(user_input, collection, st.session_state.bert_tokenizer, st.session_state.bert_model)
            # æ‰¾éšæœºå‚è€ƒè®ºæ–‡
            random_doc = find_random_article(collection)

            st.write("æ­£åœ¨ç”Ÿæˆé£æ ¼è¿ç§»ç»“æœï¼Œè¯·ç¨å€™...")

            try:
                adjusted_similar = adjust_writing_style_local(user_input, ref_doc.get('content', '') if ref_doc else '')
                adjusted_random = adjust_writing_style_local(user_input, random_doc.get('content', '') if random_doc else '')
            except Exception as e:
                st.error(f"é£æ ¼è¿ç§»é”™è¯¯: {e}")
                return

            st.subheader(">>> [æ–¹æ³•A] ä½¿ç”¨ç›¸ä¼¼åº¦æœ€é«˜çš„å‚è€ƒè®ºæ–‡")
            st.write(f"ç›¸ä¼¼åº¦: {sim_score:.4f}")
            st.write(ref_doc.get('content', '')[:300] + "..." if ref_doc else "æ— å‚è€ƒæ–‡çŒ®")
            st.write(adjusted_similar)

            st.subheader(">>> [æ–¹æ³•B] ä½¿ç”¨éšæœºé€‰å–çš„å‚è€ƒè®ºæ–‡")
            st.write(random_doc.get('content', '')[:300] + "..." if random_doc else "æ— å‚è€ƒæ–‡çŒ®")
            st.write(adjusted_random)

        elif feature == "è¯­ä¹‰çº é”™":
            # æ‹¼å†™çº é”™
            pinyin_info = get_pinyin_with_tone(user_input)
            spelling_prompt = (
                "ä½ æ˜¯ä¸­æ–‡æ‹¼å†™çº é”™ä¸“å®¶ï¼Œè¯·æ ¹æ®æ‹¼éŸ³ä¿¡æ¯åˆ¤æ–­å¹¶çº æ­£æ–‡æœ¬ä¸­å¯èƒ½çš„æ‹¼å†™é”™è¯¯ã€‚\n"
                f"æ–‡æœ¬ï¼š{user_input}\næ‹¼éŸ³ï¼š{pinyin_info}"
            )
            spelling_result = call_local_qwen(spelling_prompt)

            if ENABLE_SELF_REFLECTION:
                reflection_prompt = (
                    "ä½ æ˜¯ä¸­æ–‡æ‹¼å†™çº é”™æ£€æŸ¥å‘˜ï¼Œè¯·æ£€æŸ¥çº é”™ç»“æœæ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œéµå¾ªæœ€å°å˜åŒ–åŸåˆ™ï¼Œé¿å…å¼•å…¥æ–°é”™è¯¯ã€‚\n"
                    f"åŸå¥: {user_input}\nåˆå§‹çº é”™ç»“æœ: {spelling_result}\nè¯·è¾“å‡ºæœ€ç»ˆæ­£ç¡®çš„å¥å­:"
                )
                spelling_result = call_local_qwen(reflection_prompt)

            syntax_report = generate_syntax_analysis(spelling_result)
            grammar_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡è¯­ç—…çº é”™æ¨¡å‹ï¼Œè¯·çº æ­£ä»¥ä¸‹å¥å­ä¸­çš„è¯­æ³•é—®é¢˜ï¼Œéµå¾ªæœ€å°æ”¹åŠ¨åŸåˆ™ã€‚\n"
                f"å¥å­ï¼š{spelling_result}\nè¯­æ³•åˆ†æï¼š\n{syntax_report}"
            )
            grammar_result = call_local_qwen(grammar_prompt)

            if ENABLE_SELF_REFLECTION:
                grammar_reflection_prompt = (
                    "ä½ æ˜¯è¯­ç—…æ£€æŸ¥å‘˜ï¼Œè¯·æ£€æŸ¥çº é”™ç»“æœæ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œéµå¾ªæœ€å°å˜åŒ–åŸåˆ™ï¼Œé¿å…å¼•å…¥æ–°é”™è¯¯ã€‚\n"
                    f"åŸå¥: {user_input}\nåˆå§‹çº é”™ç»“æœ: {grammar_result}\nè¯·è¾“å‡ºæœ€ç»ˆæ­£ç¡®çš„å¥å­:"
                )
                grammar_result = call_local_qwen(grammar_reflection_prompt)

            st.subheader("ã€è¯­ä¹‰çº é”™ç»“æœã€‘")
            st.write(grammar_result)

if __name__ == "__main__":
    main()

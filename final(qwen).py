import streamlit as st
import subprocess
from pypinyin import pinyin, Style
import spacy
import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel
from pymongo import MongoClient
from pymongo.server_api import ServerApi
import certifi
import random
from highlight import diff_highlight
import hashlib

# ================== é…ç½® & å…¨å±€å¸¸é‡ ==================
QWEN_MODEL_NAME = "qwen2.5:14b"
MODEL_TIMEOUT = 180


# ================== èµ„æºåŠ è½½ä¸ç¼“å­˜ ==================
@st.cache_resource
def connect_mongodb():
    uri = (
        "mongodb://2068432802:lzq520796@"
        "cluster0-shard-00-00.tmy62.mongodb.net:27017,"
        "cluster0-shard-00-01.tmy62.mongodb.net:27017,"
        "cluster0-shard-00-02.tmy62.mongodb.net:27017/"
        "?ssl=true&replicaSet=atlas-j16zyw-shard-0&authSource=admin"
        "&retryWrites=true&w=majority&appName=Cluster0"
    )
    try:
        client = MongoClient(uri, tlsCAFile=certifi.where(), server_api=ServerApi('1'))
        client.admin.command('ping')
        print("æˆåŠŸè¿æ¥åˆ° MongoDBï¼")
        return client
    except Exception as e:
        st.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None
def hash_password(password: str) -> str:
    """å¯†ç å“ˆå¸Œ"""
    return hashlib.sha256(password.encode()).hexdigest()


def get_user_collection():
    """è·å–ç”¨æˆ·é›†åˆ"""
    client = st.session_state.mongodb_client
    db = client.get_database("paper")
    return db.get_collection("users")


def register_user(username, password):
    """æ³¨å†Œæ–°ç”¨æˆ·"""
    users = get_user_collection()
    if users.find_one({"username": username}):
        return False, "ç”¨æˆ·åå·²å­˜åœ¨"
    users.insert_one({"username": username, "password": hash_password(password)})
    return True, "æ³¨å†ŒæˆåŠŸ"


def authenticate_user(username, password):
    """éªŒè¯ç”¨æˆ·"""
    users = get_user_collection()
    user = users.find_one({"username": username})
    if user and user["password"] == hash_password(password):
        return True
    return False


def login_or_register():
    """ç™»å½• / æ³¨å†Œç•Œé¢"""
    st.markdown("### ğŸ” ç”¨æˆ·ç™»å½•/æ³¨å†Œ")
    mode = st.radio("é€‰æ‹©æ“ä½œ", ["ç™»å½•", "æ³¨å†Œ"], horizontal=True)
    username = st.text_input("ç”¨æˆ·å")
    password = st.text_input("å¯†ç ", type="password")

    if mode == "æ³¨å†Œ":
        if st.button("æ³¨å†Œ"):
            success, msg = register_user(username, password)
            if success:
                st.success(msg)
                st.session_state.username = username
                st.rerun()
            else:
                st.error(msg)
    else:  # ç™»å½•æ¨¡å¼
        if st.button("ç™»å½•"):
            if authenticate_user(username, password):
                st.session_state.username = username
                st.success(f"æ¬¢è¿å›æ¥ï¼Œ{username}ï¼")
                st.rerun()
            else:
                st.error("ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ï¼")


def check_login():
    """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
    if "username" not in st.session_state:
        login_or_register()
        st.stop()
def login_register():
    if 'users' not in st.session_state:
        st.session_state.users = {}
    st.sidebar.title("ç”¨æˆ·ç™»å½• / æ³¨å†Œ")
    mode = st.sidebar.selectbox("é€‰æ‹©æ“ä½œ", ["ç™»å½•", "æ³¨å†Œ"])
    username = st.sidebar.text_input("ç”¨æˆ·å")
    password = st.sidebar.text_input("å¯†ç ", type="password")
    if st.sidebar.button(mode):
        if not username or not password:
            st.sidebar.warning("ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º")
            return False
        if mode == "æ³¨å†Œ":
            if username in st.session_state.users:
                st.sidebar.error("ç”¨æˆ·å·²å­˜åœ¨")
            else:
                st.session_state.users[username] = hash_password(password)
                st.sidebar.success("æ³¨å†ŒæˆåŠŸï¼Œè¯·ç™»å½•")
        else:
            if username not in st.session_state.users:
                st.sidebar.error("ç”¨æˆ·ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ³¨å†Œ")
            elif st.session_state.users[username] != hash_password(password):
                st.sidebar.error("å¯†ç é”™è¯¯")
            else:
                st.session_state.username = username
                st.sidebar.success(f"æ¬¢è¿ {username}ï¼")
                return True
    return 'username' in st.session_state

@st.cache_resource
def initialize_bert():
    try:
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        model = BertModel.from_pretrained('bert-base-uncased')
        print("BERT æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
        return tokenizer, model
    except Exception as e:
        st.error(f"BERT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None


@st.cache_resource
def load_spacy_model():
    try:
        nlp = spacy.load("zh_core_web_sm")
        print("Spacy æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
        return nlp
    except Exception as e:
        st.error(f"Spacy æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


# ================== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ==================
def call_local_qwen(prompt: str) -> str:
    try:
        process = subprocess.run(
            ['ollama', 'run', QWEN_MODEL_NAME],
            input=prompt,
            capture_output=True,
            text=True,
            encoding='utf-8',
            check=True,
            timeout=MODEL_TIMEOUT
        )
        return process.stdout.strip()
    except FileNotFoundError:
        return "[æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥] 'ollama' å‘½ä»¤æœªæ‰¾åˆ°ã€‚"
    except subprocess.CalledProcessError as e:
        return f"[æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥] {e.stderr.strip()}"
    except subprocess.TimeoutExpired:
        return f"[æœ¬åœ°æ¨¡å‹è°ƒç”¨è¶…æ—¶] {MODEL_TIMEOUT} ç§’å†…æœªè¿”å›ã€‚"
    except Exception as e:
        return f"[æœ¬åœ°æ¨¡å‹è°ƒç”¨å‡ºé”™] {str(e)}"


def get_embedding(text, tokenizer, model):
    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad(): outputs = model(**tokens)
    return outputs.last_hidden_state[:, 0, :]

def find_most_similar(input_text, collection, tokenizer, model):
    input_embedding = get_embedding(input_text, tokenizer, model)
    max_similarity = -1
    most_similar_article = None
    for doc in collection.find().limit(100):
        if 'content' not in doc or not doc['content']: continue
        db_embedding = get_embedding(doc['content'], tokenizer, model)
        similarity = F.cosine_similarity(input_embedding, db_embedding).item()
        if similarity > max_similarity:
            max_similarity = similarity
            most_similar_article = doc
    return most_similar_article, max_similarity


def find_random_article(collection):
    pipeline = [{"$match": {"content": {"$exists": True}}}, {"$sample": {"size": 1}}]
    try:
        result = list(collection.aggregate(pipeline))
        return result[0] if result else None
    except Exception:
        docs = list(collection.find({"content": {"$exists": True}}))
        return random.choice(docs) if docs else None


def adjust_writing_style_local(input_text, reference_text):
    prompt = (
        f"ä½ æ˜¯ä¸€åå­¦æœ¯å†™ä½œä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®å‚è€ƒè®ºæ–‡è°ƒæ•´æ–‡æœ¬é£æ ¼ã€‚è¯·å°½æœ€å¤§å¯èƒ½ä¿æŒåŸæ–‡å†…å®¹ï¼Œä¸è¦å¢åŠ åŸæ–‡æ²¡æœ‰çš„å†…å®¹ï¼Œåªä¿®æ”¹å†™ä½œé£æ ¼ï¼Œä½†ä¿®æ”¹ç”¨è¯ã€å¥å¼å’Œç»“æ„ä»¥å°½é‡åŒ¹é…å‚è€ƒè®ºæ–‡çš„å­¦æœ¯é£æ ¼ã€‚åªå€Ÿé‰´å‚è€ƒè®ºæ–‡çš„å­¦æœ¯é£æ ¼ï¼Œä¸å€Ÿé‰´å‚è€ƒè®ºæ–‡çš„å†…å®¹ã€‚\nå‚è€ƒè®ºæ–‡ç‰‡æ®µï¼š\n{reference_text[:2000]}...\n\nè¯·ä¿®æ”¹ä»¥ä¸‹æ–‡ç« ä½¿å…¶ç¬¦åˆå‚è€ƒè®ºæ–‡çš„å­¦æœ¯é£æ ¼ï¼š\n{input_text}")
    return call_local_qwen(prompt)


def get_pinyin_with_tone(text):
    pinyin_list = pinyin(text, style=Style.TONE3)
    return " ".join([item[0] for item in pinyin_list])


def generate_syntax_analysis(text, nlp_model):
    doc = nlp_model(text)
    lines = ["[ä¾å­˜å¥æ³•åˆ†æ]"]
    for token in doc: lines.append(
        f"è¯è¯­: {token.text:<5} è¯æ€§: {token.pos_:<5} ä¾å­˜å…³ç³»: {token.dep_:<10} æ”¯é…è¯: {token.head.text}")
    lines.append("\n[å‘½åå®ä½“è¯†åˆ«]")
    if not doc.ents:
        lines.append("æœªè¯†åˆ«åˆ°å‘½åå®ä½“")
    else:
        for ent in doc.ents: lines.append(
            f"å®ä½“: {ent.text:<15} ç±»å‹: {ent.label_:<10} ä½ç½®: ({ent.start_char}-{ent.end_char})")
    return "\n".join(lines)


def summarize_user_focus_area():
    user_texts = [m["input"] for m in st.session_state[history_key] if m["type"] in ["é£æ ¼è¿ç§»", "è¯­ä¹‰çº é”™"]]
    if not user_texts: return "æœªè·å–åˆ°ç”¨æˆ·å†å²æé—®"
    prompt = ("è¯·é˜…è¯»ä»¥ä¸‹ç”¨æˆ·çš„æé—®å†å²ï¼Œæ€»ç»“å‡ºå…¶å…³æ³¨çš„å­¦æœ¯é¢†åŸŸï¼Œç›´æ¥è¾“å‡º1-2ä¸ªç®€æ´å…³é”®è¯ï¼š\n" + "\n".join(user_texts))
    interest_tags = call_local_qwen(prompt)
    st.session_state["interest_tags"] = interest_tags
    return interest_tags
def generate_paper_overview_from_history(chat_history):
    history_texts = [m["input"] for m in chat_history if m["type"] in ["é£æ ¼è¿ç§»", "æ–‡æœ¬çº é”™"]]
    if not history_texts:
        return {
            "ç ”ç©¶ç›®çš„": "æš‚æ— ",
            "ç›¸å…³å·¥ä½œ": "æš‚æ— ",
            "å®éªŒå†…å®¹": "æš‚æ— ",
            "ç»“è®º": "æš‚æ— ",
            "æœªæ¥æ–¹å‘": "æš‚æ— "
        }
    overview_prompt = (
        "ä½ æ˜¯ä¸€åå­¦æœ¯åŠ©æ‰‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šæä¾›å¤šæ®µç”¨æˆ·çš„å†™ä½œå†…å®¹ï¼ˆæ¥è‡ªç”¨æˆ·å†å²æé—®ï¼‰ï¼Œè¯·ä½ å¸®æˆ‘æ ¹æ®è¿™äº›å†…å®¹ï¼Œæå–å‡ºä¸€ç¯‡å®Œæ•´è®ºæ–‡åº”åŒ…æ‹¬çš„äº”ä¸ªéƒ¨åˆ†ï¼š"
        "ç ”ç©¶ç›®çš„ã€ç›¸å…³å·¥ä½œã€å®éªŒå†…å®¹ã€ç»“è®ºã€æœªæ¥æ–¹å‘ã€‚\n\n"
        "å¦‚æœæ— æ³•ä»æ–‡æœ¬ä¸­æå–æŸä¸€éƒ¨åˆ†ï¼Œè¯·å†™â€œæš‚æ— â€ã€‚è¯·ä½¿ç”¨å¦‚ä¸‹æ ¼å¼è¾“å‡ºï¼š\n\n"
        "ç ”ç©¶ç›®çš„ï¼š...\nç›¸å…³å·¥ä½œï¼š...\nå®éªŒå†…å®¹ï¼š...\nç»“è®ºï¼š...\næœªæ¥æ–¹å‘ï¼š...\n\n"
        f"ä»¥ä¸‹æ˜¯ç”¨æˆ·å†å²å†…å®¹ï¼š\n{chr(10).join(history_texts)}"
    )
    response = call_local_qwen(overview_prompt)
    sections = {
        "ç ”ç©¶ç›®çš„": "æš‚æ— ",
        "ç›¸å…³å·¥ä½œ": "æš‚æ— ",
        "å®éªŒå†…å®¹": "æš‚æ— ",
        "ç»“è®º": "æš‚æ— ",
        "æœªæ¥æ–¹å‘": "æš‚æ— "
    }
    for key in sections:
        if f"{key}ï¼š" in response:
            try:
                content = response.split(f"{key}ï¼š")[1].split("\n")[0].strip()
                if content:
                    sections[key] = content
            except:
                pass
    return sections

def generate_personalized_suggestions(focus_area, user_input_text):
    prompt = (
        f"å·²çŸ¥ç”¨æˆ·å…³æ³¨çš„å­¦ç§‘é¢†åŸŸåŒ…æ‹¬ï¼š{focus_area}ã€‚\nä»¥ä¸‹æ˜¯ç”¨æˆ·åœ¨è¯¥é¢†åŸŸå†…æ’°å†™çš„æ–‡æœ¬å†…å®¹:\n{user_input_text}\nè¯·ç»“åˆç”¨æˆ·å…³æ³¨çš„å­¦æœ¯é¢†åŸŸï¼Œæ ¹æ®è¯¥é¢†åŸŸå­¦æœ¯çš„å†™ä½œè§„èŒƒï¼ŒæŒ‡å‡ºç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ä¸­å­˜åœ¨çš„ä¸€äº›é—®é¢˜ï¼Œå¹¶ä»ä¸ç”¨æˆ·è¾“å…¥ä¸­ä¸¾å‡ºä¸€äº›ä¾‹å­å°è¯ï¼Œæœ€åå†æå‡ºç”¨æˆ·åœ¨æ–‡ç« ç»“æ„ã€å†™ä½œé£æ ¼ã€é€»è¾‘è¡¨è¾¾æˆ–æœ¯è¯­ä½¿ç”¨æ–¹é¢éœ€è¦é€æ­¥æå‡çš„æ–¹å‘ï¼Œå¹¶ç”¨'ä½ 'ä¸ºç§°å‘¼ã€‚")
    return call_local_qwen(prompt)

def main():
    st.set_page_config(page_title="å­¦æœ¯å†™ä½œæ™ºèƒ½åŠ©æ‰‹", layout="wide")

    if not login_register():
        st.info("è¯·å…ˆç™»å½•æˆ–æ³¨å†Œ")
        return

    username = st.session_state.username
    history_key = f"chat_history_{username}"
    if history_key not in st.session_state:
        st.session_state[history_key] = []

    st.markdown(f"<h1 style='text-align:center; color:#4A90E2;'>å­¦æœ¯å†™ä½œæ™ºèƒ½åŠ©æ‰‹ - ç”¨æˆ·ï¼š{username}</h1>", unsafe_allow_html=True)

    # ä½ çš„åˆå§‹åŒ–ä»£ç 
    if "mongodb_client" not in st.session_state:
        st.session_state.mongodb_client = connect_mongodb()
    if "bert_tokenizer" not in st.session_state:
        st.session_state.bert_tokenizer, st.session_state.bert_model = initialize_bert()
    if "spacy_model" not in st.session_state:
        st.session_state.spacy_model = load_spacy_model()
    if not all([st.session_state.mongodb_client, st.session_state.bert_tokenizer, st.session_state.spacy_model]):
        st.warning("æ ¸å¿ƒç»„ä»¶åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç»ˆç«¯æ—¥å¿—ã€‚")
        st.stop()

    with st.sidebar:
        st.title("åŠŸèƒ½é€‰æ‹©")
        feature = st.radio("è¯·é€‰æ‹©åŠŸèƒ½", ["æ–‡æœ¬çº é”™", "é£æ ¼è¿ç§»", "ä¸ªæ€§åŒ–å»ºè®®"], key="feature_selection")
        enable_self_reflection = st.toggle("è‡ªæˆ‘åæ€åŠŸèƒ½", value=True) if feature == "æ–‡æœ¬çº é”™" else False
        st.markdown("---")
        st.subheader("èŠå¤©å†å²")
        for i, chat in enumerate(st.session_state[history_key]):
            st.markdown(f"**{i + 1}. [{chat['type']}]** {chat['input'][:20]}...")

    for chat in st.session_state[history_key]:
        with st.chat_message("user"):
            st.markdown(chat["input"])
        output = chat.get("highlight_html") or chat.get("corrected_output") or chat.get("adjusted_output") or chat.get("suggestions") or ""
        with st.chat_message("assistant"):
            if chat.get("highlight_html"):
                st.markdown(chat["highlight_html"], unsafe_allow_html=True)
            else:
                st.markdown(output)

    if input_text := st.chat_input("è¯·è¾“å…¥æ‚¨è¦å¤„ç†çš„æ–‡æœ¬..."):
        st.session_state[history_key].append({"type": feature, "input": input_text})

        with st.spinner("AI æ­£åœ¨å¤„ç†ï¼Œè¯·ç¨å€™..."):
            if feature == "æ–‡æœ¬çº é”™":
                pinyin_info = get_pinyin_with_tone(input_text)
                spelling_prompt = (
                    f"ä½ æ˜¯ä¸­æ–‡æ‹¼å†™çº é”™ä¸“å®¶ï¼Œä¸éœ€è¦åˆ¤æ–­æ–‡æœ¬å†…å®¹æ˜¯å¦åˆç†ï¼Œè€Œæ˜¯æ ¹æ®æ‹¼éŸ³ä¿¡æ¯ï¼Œåˆ¤æ–­å¹¶çº æ­£ä¸­æ–‡æ–‡æœ¬ä¸­å¯èƒ½å­˜åœ¨çš„æ‹¼å†™é”™è¯¯,å¦‚æœæ–‡æœ¬ä¸­æœ‰æ‹¼å†™é”™è¯¯ï¼Œè¯·ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å¥å­ï¼Œæ— éœ€æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ï¼Œå¦‚æœè¾“å…¥çš„å¥å­ä¸­ä¸å­˜åœ¨æ‹¼å†™é”™è¯¯ï¼Œåˆ™ç›´æ¥è¾“å‡ºåŸå¥å³å¯ã€‚æ–‡æœ¬ï¼š{input_text}\næ‹¼éŸ³ï¼š{pinyin_info}è¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®çš„å¥å­,ä¸è¦ç»™å‡ºå…¶ä»–å¤šä½™æ–‡å­—:")
                spelling_result = call_local_qwen(spelling_prompt)

                if enable_self_reflection:
                    reflection_prompt = (
                        f"è¯·æ£€æŸ¥ä»¥ä¸‹çº é”™ç»“æœæ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š\n1. æ˜¯å¦è§£å†³äº†åŸå¥ä¸­çš„æ‰€æœ‰æ‹¼å†™é—®é¢˜\n2. æ˜¯å¦éµå¾ªäº†æœ€å°å˜åŒ–åŸåˆ™\n3. æ˜¯å¦å¼•å…¥äº†æ–°çš„é”™è¯¯\n4. å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºæ”¹è¿›åçš„å¥å­ï¼Œæ— éœ€è§£é‡Š;å¦‚æœç»“æœæ­£ç¡®ï¼Œè¯·ç›´æ¥è¾“å‡ºåŸå¥\nåŸå¥: {input_text}\nåˆå§‹çº é”™ç»“æœ: {spelling_result}\n\nè¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®çš„å¥å­,ä¸è¦ç»™å‡ºå…¶ä»–å¤šä½™æ–‡å­—:")
                    spelling_result = call_local_qwen(reflection_prompt)

                if len(input_text) <= 150:
                    syntax_report = generate_syntax_analysis(spelling_result, st.session_state.spacy_model)
                    grammar_prompt = (
                        f"ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡è¯­ç—…çº é”™æ¨¡å‹ï¼Œå‚è€ƒæä¾›çš„å¥æ³•åˆ†ææŠ¥å‘Šï¼Œä½ éœ€è¦è¯†åˆ«å¹¶çº æ­£è¾“å…¥çš„æ–‡æœ¬ä¸­å¯èƒ½å«æœ‰çš„è¯­ç—…é”™è¯¯å¹¶è¾“å‡ºæ­£ç¡®çš„æ–‡æœ¬ï¼Œçº æ­£æ—¶å°½å¯èƒ½å‡å°‘å¯¹åŸæ–‡æœ¬çš„æ”¹åŠ¨ï¼Œå¹¶ç¬¦åˆæœ€å°å˜åŒ–åŸåˆ™ï¼Œå³ä¿è¯è¿›è¡Œçš„ä¿®æ”¹éƒ½æ˜¯æœ€å°ä¸”å¿…è¦çš„ï¼Œä½ åº”è¯¥é¿å…å¯¹æ–‡ç« ç»“æ„æˆ–è¯æ±‡è¡¨è¾¾é£æ ¼è¿›è¡Œçš„ä¿®æ”¹ã€‚è¦æ±‚ç›´æ¥è¾“å‡ºæ²¡æœ‰è¯­æ³•é”™è¯¯çš„å¥å­ï¼Œæ— éœ€æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ï¼Œå¦‚æœè¾“å…¥çš„å¥å­ä¸­ä¸å­˜åœ¨è¯­æ³•é”™è¯¯ï¼Œåˆ™ç›´æ¥è¾“å‡ºåŸå¥å³å¯ã€‚å¥å­ï¼š{spelling_result}\nè¯­æ³•åˆ†æç»“æœï¼š\n{syntax_report}è¯·ç›´æ¥è¾“å‡ºæ­£ç¡®çš„æ–‡æœ¬,ä¸è¦ç»™å‡ºå…¶ä»–å¤šä½™æ–‡å­—:")
                    grammar_result = call_local_qwen(grammar_prompt)
                else:
                    grammar_prompt = (
                        f"ä½ æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¸­æ–‡è¯­ç—…çº é”™æ¨¡å‹ï¼Œä½ éœ€è¦è¯†åˆ«å¹¶çº æ­£è¾“å…¥çš„æ–‡æœ¬ä¸­å¯èƒ½å«æœ‰çš„è¯­ç—…é”™è¯¯å¹¶è¾“å‡ºæ­£ç¡®çš„æ–‡æœ¬ï¼Œçº æ­£æ—¶å°½å¯èƒ½å‡å°‘å¯¹åŸæ–‡æœ¬çš„æ”¹åŠ¨ï¼Œå¹¶ç¬¦åˆæœ€å°å˜åŒ–åŸåˆ™ï¼Œå³ä¿è¯è¿›è¡Œçš„ä¿®æ”¹éƒ½æ˜¯æœ€å°ä¸”å¿…è¦çš„ï¼Œä½ åº”è¯¥é¿å…å¯¹æ–‡ç« ç»“æ„æˆ–è¯æ±‡è¡¨è¾¾é£æ ¼è¿›è¡Œçš„ä¿®æ”¹ã€‚è¦æ±‚ç›´æ¥è¾“å‡ºæ²¡æœ‰è¯­æ³•é”™è¯¯çš„å¥å­ï¼Œæ— éœ€æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜ï¼Œå¦‚æœè¾“å…¥çš„å¥å­ä¸­ä¸å­˜åœ¨è¯­æ³•é”™è¯¯ï¼Œåˆ™ç›´æ¥è¾“å‡ºåŸå¥å³å¯ã€‚å¥å­ï¼š{spelling_result}\nè¯·ç›´æ¥è¾“å‡ºæ­£ç¡®çš„æ–‡æœ¬,ä¸è¦ç»™å‡ºå…¶ä»–å¤šä½™æ–‡å­—:")
                    grammar_result = call_local_qwen(grammar_prompt)

                if enable_self_reflection:
                    grammar_reflection_prompt = (
                        f"ä½ æ˜¯è¯­ç—…æ£€æŸ¥å‘˜ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹çº é”™ç»“æœæ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š\n1. æ˜¯å¦è§£å†³äº†åŸå¥ä¸­çš„æ‰€æœ‰è¯­ç—…é—®é¢˜\n2. æ˜¯å¦éµå¾ªäº†æœ€å°å˜åŒ–åŸåˆ™\n3. æ˜¯å¦å¼•å…¥äº†æ–°çš„é”™è¯¯\n4. å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºæ”¹è¿›åçš„å¥å­ï¼Œæ— éœ€è§£é‡Šï¼›å¦‚æœç”¨æˆ·åˆå§‹çº é”™ç»“æœæ­£ç¡®ï¼Œè¯·ç›´æ¥è¾“å‡ºåˆå§‹çº é”™ç»“æœ,ä¸éœ€è¦è¯´æ˜\nåŸå¥: {input_text}\nåˆå§‹çº é”™ç»“æœ: {grammar_result}\n\nè¯·ç›´æ¥è¾“å‡ºæœ€ç»ˆæ­£ç¡®çš„å¥å­ï¼Œä¸éœ€è¦å…¶ä»–å¤šä½™æ–‡å­—:")
                    grammar_result = call_local_qwen(grammar_reflection_prompt)

                # å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° chat_history
                # é«˜äº®ç‰ˆæœ¬
                highlight_html = diff_highlight(input_text, grammar_result)
                st.session_state[history_key][-1]["corrected_output"] = grammar_result
                st.session_state[history_key][-1]["highlight_html"] = highlight_html

                # ä½ çš„æ–‡æœ¬çº é”™å¤„ç†é€»è¾‘ï¼Œä½¿ç”¨ st.session_state[history_key] ä»£æ›¿ chat_history
                ...
                # ä¾‹ï¼š st.session_state[history_key][-1]["corrected_output"] = grammar_result
            elif feature == "é£æ ¼è¿ç§»":
                db = st.session_state.mongodb_client.get_database("paper")
                collection = db.get_collection("papers")
                ref_doc, sim_score = find_most_similar(input_text, collection, st.session_state.bert_tokenizer,
                                                       st.session_state.bert_model)

                if ref_doc:
                    adjusted_similar = adjust_writing_style_local(input_text, ref_doc.get('content', ''))
                    output_message = f"**æ ¹æ®é£æ ¼ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸‰ç¯‡è®ºæ–‡ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {sim_score:.4f}ï¼‰ä¼˜åŒ–åï¼š**\n\n---\n\n{adjusted_similar}"
                else:
                    output_message = "æŠ±æ­‰ï¼Œæ•°æ®åº“ä¸­æœªèƒ½æ‰¾åˆ°ç›¸ä¼¼çš„å‚è€ƒè®ºæ–‡ã€‚è¯·å°è¯•å…¶ä»–æ–‡æœ¬æˆ–æ£€æŸ¥æ•°æ®åº“ã€‚"
                # å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° chat_history
                st.session_state[history_key][-1]["adjusted_output"] = output_message

                # åˆ†æ”¯ä¸‰ï¼šä¸ªæ€§åŒ–å»ºè®®

            elif feature == "ä¸ªæ€§åŒ–å»ºè®®":
                if not any(chat['type'] in ["é£æ ¼è¿ç§»", "æ–‡æœ¬çº é”™"] for chat in st.session_state[history_key]):
                    suggestions = "æš‚æ— å†å²è®°å½•ï¼Œæ— æ³•ç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®ã€‚è¯·å…ˆä½¿ç”¨â€œæ–‡æœ¬çº é”™â€æˆ–â€œé£æ ¼è¿ç§»â€åŠŸèƒ½ã€‚"
                else:
                    overview = generate_paper_overview_from_history(st.session_state[history_key])
                    overview_text = "\n".join([f"{k}ï¼š{v}" for k, v in overview.items()])
                    prompt = (
                        f"ä»¥ä¸‹æ˜¯ç”¨æˆ·å½“å‰æ’°å†™çš„æ–‡æœ¬ï¼š\n{input_text}\n\n"
                        f"ä»¥ä¸‹æ˜¯æ ¹æ®ç”¨æˆ·å†å²å†™ä½œæå–çš„è®ºæ–‡æ¦‚è§ˆä¿¡æ¯ï¼š\n{overview_text}\n\n"
                        "è¯·ä½ ç»“åˆç”¨æˆ·å½“å‰æ–‡æœ¬ä¸è¿™äº›æ¦‚è§ˆä¿¡æ¯ï¼ŒæŒ‡å‡ºå…¶æ–‡æœ¬å†…å®¹å­˜åœ¨çš„ä¸»è¦é—®é¢˜ï¼Œ"
                        "å¹¶æä¾›è¯¦ç»†å»ºè®®å’Œä¿®æ”¹æ–¹å‘ã€‚ä½ å¯ä»¥å¼•ç”¨æ¦‚è§ˆå†…å®¹ä½œä¸ºå‚è€ƒæ¥åˆ¤æ–­å½“å‰æ–‡æœ¬æ˜¯å¦åç¦»åŸæ„æˆ–é£æ ¼ã€‚è¯·ç›´æ¥ç”¨â€œä½ â€æ¥ç§°å‘¼ç”¨æˆ·ï¼Œæ ¼å¼æ¸…æ™°ã€æ¡ç†æ˜ç¡®ã€‚"
                    )
                    suggestions = call_local_qwen(prompt)
                # å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° chat_history
                st.session_state[history_key][-1]["suggestions"] = suggestions
        # å¤„ç†å®Œæ¯•ååˆ·æ–°ç•Œé¢å¹¶é¿å…æ— é™å¾ªç¯ï¼Œå…ˆæ ‡è®°å†rerun
        st.session_state['just_updated'] = True

    if st.session_state.get('just_updated', False):
        st.session_state['just_updated'] = False
        st.rerun()

if __name__ == "__main__":
    main()
